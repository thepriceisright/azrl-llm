import os
import re
import json
import random
import time
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass
import traceback
import datetime  # Added for timestamped logs

from utils import get_logger, get_config
from src.llm import get_model_service, get_prompt_manager
from src.buffer import get_task_buffer
from src.executor import get_executor
from src.rl import get_reward_calculator, get_rl_trainer, RLExample


@dataclass
class TaskProposal:
    """A task proposal generated by the Proposer."""
    task_type: str
    program: Optional[str] = None
    input_data: Optional[Any] = None
    output: Optional[Any] = None
    examples: Optional[List[Tuple[Any, Any]]] = None
    metaprompt: Optional[str] = None
    is_valid: bool = False
    error_reason: Optional[str] = None


class AZRPipeline:
    """
    Main orchestrator for the Absolute Zero Reinforcement Learning pipeline.
    
    Implements Algorithm 1 from the AZR paper.
    """
    def __init__(self):
        """Initialize the AZR pipeline."""
        self.logger = get_logger("azr_pipeline")
        self.config = get_config()
        
        # Get components
        self.logger.log("Initializing model service...")
        self.model_service = get_model_service()
        self.logger.log("Initializing prompt manager...")
        self.prompt_manager = get_prompt_manager()
        self.logger.log("Initializing task buffer...")
        self.task_buffer = get_task_buffer()
        self.logger.log("Initializing executor...")
        self.executor = get_executor()
        self.logger.log("Initializing reward calculator...")
        self.reward_calculator = get_reward_calculator()
        self.logger.log("Initializing RL trainer...")
        self.rl_trainer = get_rl_trainer()
        
        # Get configuration
        self.task_types = self.config.get("task_types", ["abduction", "deduction", "induction"])
        self.k_references = self.config.get("training.k_references", 6)
        self.n_samples = self.config.get("training.n_samples", 8)
        self.checkpoint_interval = self.config.get("training.checkpoint_interval", 50)
        self.rollout_temp = self.config.get("training.rollout_temp", 1.0)
        self.rollout_top_p = self.config.get("training.rollout_top_p", 1.0)
        self.progress_interval = self.config.get("logging.progress_interval", 60)  # Seconds between progress updates
        
        # Initialize task buffer if empty
        self.logger.log("Initializing task buffer with seed examples...")
        self.task_buffer.initialize_with_seed_examples()
        self.logger.log("Pipeline initialization complete.")
    
    def _parse_abduction_deduction_proposal(self, text: str, task_type: str) -> TaskProposal:
        """
        Parse a proposal for abduction or deduction tasks.
        
        Args:
            text: Text generated by the proposer
            task_type: Either "abduction" or "deduction"
            
        Returns:
            TaskProposal object
        """
        proposal = TaskProposal(task_type=task_type)
        
        # Extract the function code
        code_match = re.search(r'```python(.*?)```', text, re.DOTALL)
        if not code_match:
            proposal.error_reason = "No Python code block found in the proposal"
            return proposal
        
        code = code_match.group(1).strip()
        proposal.program = code
        
        # Extract input/output based on task type
        if task_type == "abduction":
            # For abduction, look for the output
            output_match = re.search(r'Expected output:(.*?)(?:\n\n|$)', text, re.DOTALL)
            if not output_match:
                proposal.error_reason = "No expected output found in the proposal"
                return proposal
            
            output_str = output_match.group(1).strip()
            
            # Try to parse the output as a Python literal
            try:
                proposal.output = eval(output_str)
            except Exception:
                # If it can't be parsed, use it as a string
                proposal.output = output_str
        
        elif task_type == "deduction":
            # For deduction, look for the input
            input_match = re.search(r'Input:(.*?)(?:\n\n|$)', text, re.DOTALL)
            if not input_match:
                proposal.error_reason = "No input found in the proposal"
                return proposal
            
            input_str = input_match.group(1).strip()
            
            # Try to parse the input as a Python literal
            try:
                proposal.input_data = eval(input_str)
            except Exception:
                # If it can't be parsed, use it as a string
                proposal.input_data = input_str
        
        return proposal
    
    def _parse_induction_proposal(self, text: str) -> TaskProposal:
        """
        Parse a proposal for an induction task.
        
        Args:
            text: Text generated by the proposer
            
        Returns:
            TaskProposal object
        """
        proposal = TaskProposal(task_type="induction")
        
        # Extract the example pairs
        examples = []
        
        # Match all input-output pairs
        pairs_match = re.findall(r'Input:(.*?)Output:(.*?)(?=Input:|Metaprompt:|$)', text, re.DOTALL)
        
        if not pairs_match or len(pairs_match) < 2:
            proposal.error_reason = "Not enough input-output pairs found (need at least 2)"
            return proposal
        
        for input_str, output_str in pairs_match:
            input_str = input_str.strip()
            output_str = output_str.strip()
            
            # Try to parse as Python literals
            try:
                input_val = eval(input_str)
            except Exception:
                input_val = input_str
            
            try:
                output_val = eval(output_str)
            except Exception:
                output_val = output_str
            
            examples.append((input_val, output_val))
        
        proposal.examples = examples
        
        # Extract the metaprompt
        metaprompt_match = re.search(r'Metaprompt:(.*?)(?:\n\n|$)', text, re.DOTALL)
        if not metaprompt_match:
            proposal.error_reason = "No metaprompt found in the proposal"
            return proposal
        
        proposal.metaprompt = metaprompt_match.group(1).strip()
        
        return proposal
    
    def _parse_solver_output(self, task_type: str, output_text: str) -> Tuple[Any, bool]:
        """
        Parse the output from the solver.
        
        Args:
            task_type: Type of task
            output_text: Text generated by the solver
            
        Returns:
            Tuple of (solution, has_format_error)
        """
        has_format_error = False
        solution = None
        
        if task_type == "abduction":
            # For abduction, look for the input
            solution_match = re.search(r'Input:(.*?)(?:\n\n|$)', output_text, re.DOTALL)
            if not solution_match:
                has_format_error = True
                return None, has_format_error
            
            solution_str = solution_match.group(1).strip()
            
            # Try to parse as Python literal
            try:
                solution = eval(solution_str)
            except Exception:
                solution = solution_str
        
        elif task_type == "deduction":
            # For deduction, look for the output
            solution_match = re.search(r'Output:(.*?)(?:\n\n|$)', output_text, re.DOTALL)
            if not solution_match:
                has_format_error = True
                return None, has_format_error
            
            solution_str = solution_match.group(1).strip()
            
            # Try to parse as Python literal
            try:
                solution = eval(solution_str)
            except Exception:
                solution = solution_str
        
        elif task_type == "induction":
            # For induction, look for the function in a code block
            code_match = re.search(r'```python(.*?)```', output_text, re.DOTALL)
            if not code_match:
                has_format_error = True
                return None, has_format_error
            
            solution = code_match.group(1).strip()
        
        return solution, has_format_error
    
    def _generate_task_proposal(self, task_type: str) -> TaskProposal:
        """
        Generate a task proposal using the Proposer.
        
        Args:
            task_type: Type of task to propose
            
        Returns:
            TaskProposal object
        """
        # Sample K reference examples from the buffer
        reference_examples = self.task_buffer.sample_tasks(task_type, self.k_references)
        
        # Generate prompt for the proposer
        prompt = self.prompt_manager.generate_proposer_prompt(task_type, reference_examples)
        
        # Generate proposal
        raw_proposal = self.model_service.generate(
            prompt,
            temperature=self.rollout_temp,
            top_p=self.rollout_top_p
        )
        
        # Parse the proposal
        if task_type in ["abduction", "deduction"]:
            proposal = self._parse_abduction_deduction_proposal(raw_proposal, task_type)
        elif task_type == "induction":
            proposal = self._parse_induction_proposal(raw_proposal)
        else:
            raise ValueError(f"Unknown task type: {task_type}")
        
        return proposal, raw_proposal
    
    def _validate_task_proposal(self, proposal: TaskProposal) -> Tuple[bool, str, Any]:
        """
        Validate a task proposal using the executor.
        
        Args:
            proposal: TaskProposal to validate
            
        Returns:
            Tuple of (is_valid, error_reason, ground_truth)
        """
        if proposal.task_type in ["abduction", "deduction"]:
            # Check if the program is valid
            is_valid, reason = self.executor.validate_program(proposal.program)
            if not is_valid:
                return False, reason, None
            
            # Check if the program is deterministic
            if proposal.task_type == "abduction":
                # For abduction, we need to find an input that produces the output
                # We'll try to solve the problem ourselves to verify it's possible
                # For now, we just check if the program runs without errors
                is_valid, reason = self.executor.validate_program(proposal.program)
                return is_valid, reason, None
            
            elif proposal.task_type == "deduction":
                # For deduction, execute the program with the input and get the output
                is_deterministic, output1, output2 = self.executor.check_determinism(
                    proposal.program, proposal.input_data
                )
                
                if not is_deterministic:
                    return False, "Program is not deterministic", None
                
                # The ground truth is the output
                ground_truth = output1
                return True, "", ground_truth
        
        elif proposal.task_type == "induction":
            # For induction, we validate by generating a program that produces the examples
            if not proposal.examples or len(proposal.examples) < 2:
                return False, "Not enough examples (need at least 2)", None
            
            # Check if the examples are consistent (can be solved by a single function)
            # For now, we assume they are
            return True, "", None
        
        return False, "Unknown task type", None
    
    def _run_solver(self, task_type: str, task_data: Dict[str, Any]) -> Tuple[Any, bool, str]:
        """
        Run the solver to solve a task.
        
        Args:
            task_type: Type of task to solve
            task_data: Data for the task
            
        Returns:
            Tuple of (solution, has_format_error, raw_solution_text)
        """
        # Generate prompt for the solver
        prompt = self.prompt_manager.generate_solver_prompt(task_type, task_data)
        
        # Generate solution
        raw_solution = self.model_service.generate(
            prompt,
            temperature=self.rollout_temp,
            top_p=self.rollout_top_p
        )
        
        # Parse the solution
        solution, has_format_error = self._parse_solver_output(task_type, raw_solution)
        
        return solution, has_format_error, raw_solution
    
    def _verify_solution(self, 
                      task_type: str,
                      program: str,
                      solution: Any,
                      expected_output: Any,
                      input_data: Any = None,
                      examples: List[Tuple[Any, Any]] = None) -> bool:
        """
        Verify a solution using the executor.
        
        Args:
            task_type: Type of task
            program: Program code
            solution: Solution from the solver
            expected_output: Expected output for deduction tasks
            input_data: Input data for abduction tasks
            examples: Examples for induction tasks
            
        Returns:
            True if the solution is correct, False otherwise
        """
        # Use the executor to verify the solution
        is_correct, details = self.executor.verify_solution(
            task_type,
            program,
            solution,
            expected_output,
            input_data if task_type == "induction" else None
        )
        
        return is_correct
    
    def _generate_input_for_abduction(self, program: str, output: Any) -> Any:
        """
        Generate an input for an abduction task that produces the given output when the program is run on it.
        
        Args:
            program: Program code
            output: Expected output
            
        Returns:
            Input that produces the output when the program is run on it, or None if no such input can be found
        """
        # For now, we'll rely on the solver to find an input
        # In a real implementation, we might use a more sophisticated approach
        return None
    
    def run_iteration(self) -> Dict[str, Any]:
        """
        Run a single iteration of the AZ training loop.
        
        Returns:
            Dictionary of metrics from the iteration
        """
        start_time = time.time()
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.logger.log(f"[{now}] Starting AZ training iteration")
        
        # Select a random task type
        task_type = random.choice(self.task_types)
        self.logger.log(f"Selected task type: {task_type}")
        
        # Initialize metrics
        metrics = {
            "task_type": task_type,
            "proposer_examples": 0,
            "solver_examples": 0,
            "proposer_reward": 0.0,
            "solver_rewards": [],
            "solver_accuracy": 0.0,
            "buffer_sizes": self.task_buffer.get_buffer_size(),
            "training_stats": {}
        }
        
        # Generate and validate task proposal
        self.logger.log(f"[STEP 1/6] Generating task proposal for {task_type}")
        gen_start = time.time()
        proposal, raw_proposal = self._generate_task_proposal(task_type)
        gen_duration = time.time() - gen_start
        self.logger.log(f"Proposal generation completed in {gen_duration:.2f} seconds")
        
        # Validate the proposal
        self.logger.log("[STEP 2/6] Validating task proposal")
        val_start = time.time()
        is_valid, error_reason, ground_truth = self._validate_task_proposal(proposal)
        val_duration = time.time() - val_start
        self.logger.log(f"Validation completed in {val_duration:.2f} seconds")
        
        if not is_valid:
            self.logger.log(f"Task proposal is invalid: {error_reason}")
            metrics["error_reason"] = error_reason
            end_time = time.time()
            metrics["duration"] = end_time - start_time
            return metrics
        
        # For abduction, we need to find an input that produces the output
        if task_type == "abduction":
            self.logger.log("[STEP 3/6] Generating input for abduction task")
            input_data = self._generate_input_for_abduction(proposal.program, proposal.output)
            if input_data is None:
                self.logger.log("Could not find input for abduction task")
                metrics["error_reason"] = "Could not find input for abduction task"
                end_time = time.time()
                metrics["duration"] = end_time - start_time
                return metrics
            
            # Update the proposal
            proposal.input_data = input_data
        
        # Add the validated task to the buffer
        self.logger.log("[STEP 4/6] Adding task to buffer")
        if task_type == "abduction" or task_type == "deduction":
            task_data = {
                "program": proposal.program,
                "input": proposal.input_data,
                "output": proposal.output if task_type == "abduction" else ground_truth
            }
        elif task_type == "induction":
            task_data = {
                "program": None,  # We don't have a ground truth program yet
                "examples": proposal.examples,
                "metaprompt": proposal.metaprompt
            }
        
        self.task_buffer.add_task(task_type, task_data)
        
        # Run solver N times and calculate learnability reward
        self.logger.log(f"[STEP 5/6] Running solver {self.n_samples} times to estimate learnability")
        solver_results = []
        
        for i in range(self.n_samples):
            try:
                solver_start = time.time()
                self.logger.log(f"Running solver instance {i+1}/{self.n_samples}")
                
                # Run solver
                solution, has_format_error, raw_solution = self._run_solver(task_type, task_data)
                
                # Verify solution
                if has_format_error:
                    is_correct = False
                    self.logger.log(f"Solver {i+1} produced output with format error")
                else:
                    self.logger.log(f"Verifying solution from solver {i+1}")
                    is_correct = self._verify_solution(
                        task_type,
                        proposal.program,
                        solution,
                        task_data["output"] if task_type == "deduction" else proposal.output,
                        task_data["input"] if task_type == "abduction" else None,
                        task_data["examples"] if task_type == "induction" else None
                    )
                    self.logger.log(f"Solution from solver {i+1} is {'correct' if is_correct else 'incorrect'}")
                
                # Calculate solver reward
                solver_reward = self.reward_calculator.calculate_solver_reward(
                    is_correct, has_format_error
                )
                
                # Add to results
                solver_results.append({
                    "is_correct": is_correct,
                    "has_format_error": has_format_error,
                    "reward": solver_reward,
                    "solution": solution,
                    "raw_solution": raw_solution
                })
                
                metrics["solver_rewards"].append(solver_reward)
                solver_duration = time.time() - solver_start
                self.logger.log(f"Solver {i+1} completed in {solver_duration:.2f} seconds (reward: {solver_reward:.2f})")
                
            except Exception as e:
                self.logger.log(f"Error during solver {i+1} execution: {e}", level="ERROR")
                self.logger.log(traceback.format_exc(), level="ERROR")
                
                # Count as a failure
                solver_results.append({
                    "is_correct": False,
                    "has_format_error": True,
                    "reward": self.reward_calculator.calculate_solver_reward(False, True),
                    "solution": None,
                    "raw_solution": None,
                    "error": str(e)
                })
        
        # Calculate proposer reward (learnability)
        proposer_reward = self.reward_calculator.calculate_proposer_reward(solver_results)
        metrics["proposer_reward"] = proposer_reward
        
        # Calculate solver accuracy
        solver_correct = sum(1 for result in solver_results if result["is_correct"])
        solver_accuracy = solver_correct / len(solver_results) if solver_results else 0.0
        metrics["solver_accuracy"] = solver_accuracy
        
        # Create RL examples
        rl_examples = []
        
        # Add proposer example
        rl_examples.append(RLExample(
            prompt=self.prompt_manager.generate_proposer_prompt(task_type, 
                                                           self.task_buffer.sample_tasks(task_type, self.k_references)),
            completion=raw_proposal,
            reward=proposer_reward,
            role="proposer",
            task_type=task_type
        ))
        metrics["proposer_examples"] = 1
        
        # Add solver examples
        for result in solver_results:
            if result.get("raw_solution") is not None:
                rl_examples.append(RLExample(
                    prompt=self.prompt_manager.generate_solver_prompt(task_type, task_data),
                    completion=result["raw_solution"],
                    reward=result["reward"],
                    role="solver",
                    task_type=task_type
                ))
                metrics["solver_examples"] += 1
        
        # Update model weights using RL
        self.logger.log(f"[STEP 6/6] Updating model weights with {len(rl_examples)} examples")
        training_start = time.time()
        training_stats = self.rl_trainer.update(self.model_service.model, rl_examples)
        training_duration = time.time() - training_start
        metrics["training_stats"] = training_stats
        
        end_time = time.time()
        total_duration = end_time - start_time
        metrics["duration"] = total_duration
        
        self.logger.log(f"AZ training iteration completed in {total_duration:.2f} seconds")
        self.logger.log(f"Summary: proposer_reward={proposer_reward:.2f}, solver_accuracy={solver_accuracy:.2f}")
        return metrics
    
    def train(self, num_iterations: int) -> List[Dict[str, Any]]:
        """
        Run the AZ training loop for a number of iterations.
        
        Args:
            num_iterations: Number of iterations to run
            
        Returns:
            List of metrics from each iteration
        """
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.logger.log(f"[{now}] Starting AZ training for {num_iterations} iterations")
        
        all_metrics = []
        last_progress_time = time.time()
        
        for iteration in range(num_iterations):
            iteration_start_time = time.time()
            now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            self.logger.log(f"[{now}] ===== Starting iteration {iteration+1}/{num_iterations} =====")
            
            try:
                metrics = self.run_iteration()
                metrics["iteration"] = iteration + 1
                all_metrics.append(metrics)
                
                # Log metrics
                self.logger.log_metrics(metrics, step=iteration+1)
                
                # Save checkpoint periodically
                if (iteration + 1) % self.checkpoint_interval == 0:
                    checkpoint_path = self.model_service.save_checkpoint()
                    self.logger.log(f"Saved checkpoint to {checkpoint_path}")
                
                # Calculate ETA
                if len(all_metrics) > 0:
                    avg_duration = sum(m.get("duration", 0) for m in all_metrics) / len(all_metrics)
                    remaining_iterations = num_iterations - (iteration + 1)
                    eta_seconds = avg_duration * remaining_iterations
                    eta_hours = eta_seconds // 3600
                    eta_minutes = (eta_seconds % 3600) // 60
                    eta_seconds = eta_seconds % 60
                    self.logger.log(f"Estimated time remaining: {int(eta_hours)}h {int(eta_minutes)}m {int(eta_seconds)}s")
            
            except Exception as e:
                self.logger.log(f"Error during iteration {iteration+1}: {e}", level="ERROR")
                self.logger.log(traceback.format_exc(), level="ERROR")
                
                # Output a heartbeat message to show the process is still alive
                now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                self.logger.log(f"[{now}] Attempting to continue with next iteration...")
            
            # Output periodic progress updates even if nothing seems to be happening
            current_time = time.time()
            if current_time - last_progress_time > self.progress_interval:
                now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                self.logger.log(f"[{now}] PROGRESS UPDATE: Completed {iteration+1}/{num_iterations} iterations")
                last_progress_time = current_time
            
            iteration_duration = time.time() - iteration_start_time
            self.logger.log(f"Iteration {iteration+1}/{num_iterations} completed in {iteration_duration:.2f} seconds")
        
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.logger.log(f"[{now}] AZ training completed after {num_iterations} iterations")
        
        # Save final checkpoint
        checkpoint_path = self.model_service.save_checkpoint()
        self.logger.log(f"Saved final checkpoint to {checkpoint_path}")
        
        # Calculate average metrics
        if all_metrics:
            avg_proposer_reward = sum(m.get("proposer_reward", 0) for m in all_metrics) / len(all_metrics)
            avg_solver_accuracy = sum(m.get("solver_accuracy", 0) for m in all_metrics) / len(all_metrics)
            avg_duration = sum(m.get("duration", 0) for m in all_metrics) / len(all_metrics)
            
            self.logger.log(f"Training summary:")
            self.logger.log(f" - Completed iterations: {len(all_metrics)}")
            self.logger.log(f" - Average proposer reward: {avg_proposer_reward:.4f}")
            self.logger.log(f" - Average solver accuracy: {avg_solver_accuracy:.4f}")
            self.logger.log(f" - Average iteration duration: {avg_duration:.2f} seconds")
        
        return all_metrics 